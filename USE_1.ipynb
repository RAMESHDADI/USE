{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "USE-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaoStkOJxR4g"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCW5ea1CxfmH"
      },
      "source": [
        "X = pd.read_csv('/content/drive/My Drive/training_set_rel3.csv', sep='\\t',encoding='ISO-8859-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsIXiTD0xinl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6250a64f-8786-497f-e03d-1b831b8f7bba"
      },
      "source": [
        "y = X['domain1_score']\n",
        "print(y)\n",
        "X = X.dropna(axis=1)\n",
        "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0         8\n",
            "1         9\n",
            "2         7\n",
            "3        10\n",
            "4         8\n",
            "         ..\n",
            "12971    35\n",
            "12972    32\n",
            "12973    40\n",
            "12974    40\n",
            "12975    40\n",
            "Name: domain1_score, Length: 12976, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8lOrKC3xl-L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "703c9c04-520c-4c9a-a564-927981f3adbb"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay_set</th>\n",
              "      <th>essay</th>\n",
              "      <th>domain1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   essay_id  ...  domain1_score\n",
              "0         1  ...              8\n",
              "1         2  ...              9\n",
              "2         3  ...              7\n",
              "3         4  ...             10\n",
              "4         5  ...              8\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT90yPEvxoiM"
      },
      "source": [
        "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
        "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy9ClhXwxr7c"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "use= hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "aa=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuBzopnrx7m3"
      },
      "source": [
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "import keras.backend as K\n",
        "from keras.layers import Bidirectional\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"Define the model.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(300, return_sequences=True),input_shape=[1, 49152]))\n",
        "    #model.add(Bidirectional(LSTM(300, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True)))\n",
        "    model.add(Bidirectional(LSTM(64, recurrent_dropout=0.4)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI9Epzjjx-BN",
        "outputId": "2fc00413-a503-45b2-d489-69895ad4a219"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjaQAiInuAEf"
      },
      "source": [
        "def essay_to_wordlist(essay_v, remove_stopwords):\n",
        "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
        "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
        "    words = essay_v.lower().split()\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return (words)\n",
        "def essay_to_sentences(essay_v, remove_stopwords):\n",
        "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    #tokennizer=PunktSentenceTokenizer(english.pickle)\n",
        "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append((raw_sentence))\n",
        "    #print(sentences)\n",
        "    return sentences\n",
        "def makeFeatureVec(words, num_features,n):\n",
        "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
        "    featureVec = np.zeros((n,num_features),dtype=\"float32\")\n",
        "    featureVec = np.add(featureVec,use(words))   \n",
        "    #print(featureVec.shape)     \n",
        "    #featureVec = np.divide(featureVec,num_words)\n",
        "    return featureVec\n",
        "def getAvgFeatureVecs(essays, num_features,aa):\n",
        "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
        "    counter = 0\n",
        "    essayFeatureVecs = np.zeros((len(essays),num_features*96),dtype=\"float32\")\n",
        "    #essayFeatureVecs=[]\n",
        "    print(num_features*96)\n",
        "    for essay in essays:\n",
        "        #essayFeatureVecs.insert(counter,makeFeatureVec(essay,num_features,aa[counter]))\n",
        "        arr=makeFeatureVec(essay, num_features,aa[counter]).flatten()\n",
        "        ss=(num_features*96)-len(arr)\n",
        "        arr1=np.pad(arr,(0,ss),'constant', constant_values=(0, 0))\n",
        "        #print(np.add(arr1,np.zeros(num_features*np.max(aa))))\n",
        "        essayFeatureVecs[counter] = np.add(arr1,np.zeros((num_features*96),dtype=\"float\"))\n",
        "        #makeFeatureVec(essay, num_features,aa[counter]).flatten()\n",
        "        #print(essayFeatureVecs[0])\n",
        "        counter = counter + 1\n",
        "    return essayFeatureVecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1swJAcWAuOR"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import tensorflow as tf\n",
        "\n",
        "cv = KFold(n_splits = 5, shuffle = True)\n",
        "results = []\n",
        "y_pred_list = []\n",
        "\n",
        "count = 1\n",
        "for traincv, testcv in cv.split(X):\n",
        "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
        "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
        "    #print(y_train)\n",
        "    train_essays = X_train['essay']\n",
        "    test_essays = X_test['essay']\n",
        "    num_features = 512\n",
        "    min_word_count = 40\n",
        "    num_workers = 4\n",
        "    context = 10\n",
        "    downsampling = 1e-3\n",
        "    clean_train_essays = []\n",
        "    clean_test_essays=[]\n",
        "    #essays=train_essays\n",
        "    sentences=[]\n",
        "    l1=[]\n",
        "    l2=[]\n",
        "    for essay in train_essays:\n",
        "      sentences += essay_to_sentences(essay, remove_stopwords = False)\n",
        "      #print(sentences)\n",
        "    for essay in test_essays:\n",
        "      sentences += essay_to_sentences(essay, remove_stopwords = False)\n",
        "    for essay_v in train_essays:\n",
        "      l1.append(len(sent_tokenize(essay_v)))\n",
        "      clean_train_essays.append(essay_to_sentences(essay_v, remove_stopwords=True))\n",
        "      #print(clean_train_essays)\n",
        "    a=np.array(l1)\n",
        "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, num_features,a)\n",
        "    for essay_v in test_essays:\n",
        "      l2.append(len(sent_tokenize(essay_v)))\n",
        "      clean_test_essays.append(essay_to_sentences(essay_v, remove_stopwords=True))\n",
        "    b=np.array(l2)\n",
        "    testDataVecs=getAvgFeatureVecs(clean_test_essays, num_features,b)\n",
        "    trainDataVecs=np.array(trainDataVecs)\n",
        "    testDataVecs = np.array(testDataVecs)\n",
        "    trainDataVecs=np.reshape(trainDataVecs,(trainDataVecs.shape[0],1,trainDataVecs.shape[1]))\n",
        "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "    print(b)\n",
        "    lstm_model = get_model()\n",
        "    history=lstm_model.fit(trainDataVecs, y_train, batch_size=60, epochs=5)\n",
        "    y_pred=(lstm_model.predict(testDataVecs))\n",
        "    #print(y_pred)\n",
        "    # Save any one of the 5 models.\n",
        "    if count == 6:\n",
        "      break\n",
        "         #lstm_model.save('./model_weights/final_lstm.h5')\n",
        "    \n",
        "    # Round y_pred to the nearest integer.\n",
        "    y_pred = np.around(y_pred)\n",
        "    print(y_pred)\n",
        "    \n",
        "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
        "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
        "    print(\"Kappa Score: {}\".format(result))\n",
        "    results.append(result)\n",
        "\n",
        "    count += 1\n",
        "   \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWi7UCYypUB4",
        "outputId": "1ebaf1ad-5fd5-403e-e275-cca3830c0202"
      },
      "source": [
        "arr = np.array([1, 3, 2, 5, 4])\n",
        "arr1=np.zeros(5)\n",
        "print(np.add(arr,arr1))\n",
        "  \n",
        "# padding array using CONSTANT mode\n",
        "pad_arr = np.pad(arr, (0, 8), 'constant', constant_values=(0, 0))\n",
        "  \n",
        "print(pad_arr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 3. 2. 5. 4.]\n",
            "[1 3 2 5 4 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}